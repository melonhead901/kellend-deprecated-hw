Kellen Donohue, Zach Stein
Kellend, steinz
06/01/09
CSE 326 A
Project 3 – Benchmarking Explanation

We performed benchmarking in two main ways, using Java timing and Unix timing. We tested each data structure, and also tested two data structure version which implemented Java library classes. One additional data structure test used Java’s HashMap and another one used Java’s TreeMap. We performed 3 trials for each test, and each test was run on attu.cs.washington.edu. The texts we used were hamlet.txt, the-new-atlantis.txt, both of which were provided, as well as War and Peace, obtained from Project Gutenburg and cleaned of any added text, as well as that same file concatenated and appended to itself 8 times.

For the Java timing tests, the nanotime was recorded before the file inserts began, measured after the inserts finished, and outputted the difference. The same procedure was used to measure the time for the program to get the count array. The two times were averaged (arithmetic mean) for each text, and thereafter only the sum of the averages was considered. The granularity of the system clock may have introduced error in the Java test, beacuse it measured as percise as miliseconds. By performing multiple we were able to diminish the strength of this unknown, variable factor. Also, since all trials were performed on the same system, under the same test conditions, the granularity was a variable that affected all trials the same way, and so can be disregarded as a concern for the test data.

The Unix tests were performed using the Unix time command. These tests were less strict than the Java tests, simply timing how long it took to run WordCount, Correlator, and both the scripts in their entirety. Therefore, these tests do not really test just the data structures, but the entire program's run time, which is what is most important to the end user. However, the pieces of the programs not involving the specific data structures should take the same amount of time no matter what data structure is used, so the results are still comparable. Three trials of each test were performed on Attu, some using the timed_runs_correlator.sh and timed_runs_word_count.sh bash scripts. The user time was recorded. The same texts were used as inputs as in the Java tests. Interpretation is simple - lower times are better. Some errors might come from process creation times that can affect the Unix time command.

To analyze the data the average (arithmetic mean) of the three trials for each test was calculated. Then for each text, the data was “normalized”. The control run was considered to the unbalanced BST. So the analysis table lists the comparison of each method’s average time to the control, with the control listed as 100% and the other method’s time expressed as a ration in percent form. Also, we averaged these percent comparisons, and then averaged the Java and UNIX averages to obtain a metric on which we judged the data structures.

Thus, the benchmark metric measures an average of the time Java measured that the operations took, and a measure of the time it took for the whole program to run, with each measurement compared to an experimental standard, the BST. This value is given as a ratio in the form of a percent of the given method compared to the BST. A lower percent is better, because it means average time to complete the program operations was faster. A score of less than 100% would mean that the method was faster on average at processing the files, for the test data, than BST, while a score of over 100% would mean the method was on average slower. 
 
The method of benchmarking is optimal because it combines positives of both UNIX and Java timing, and allows comparison of the sorting methods to the provided Shell scripts. It does however make certain assumptions about what datasets the methods will be used on. It assumes half of the data the sets will be used on is the length of an average play or novel, a quarter will be on data the size one of the largest books in a Cyrillic alpha bet, and a quarter of the data will be extremely large. The importance of each data sized could be altered by using a weighted average. A strict linear weight was used, because while most of the data will probably be about the length of Hamlet or The New Atlantis we felt it was still important that the method we chose as best scaled nicely to larger sets, thus larger data sets were given weight as well.

Out of the data structures that we did implement, the splay tree and hash table were the most competitive. Additionally, the splay tree could probably be optimized further as discussed in readme questions 6 and 7 by creating a meta root node to remove the size of 2 case and by experimenting with the default stack size (perhaps dynamically determining the size based on likely node depths - but these calculations might actually just slow the program down [as was likely the case with my few attempts to do this]). A non-recursive AVL implementation might also be more competitive.

An interesting anomaly was the performance of Hashtable and Splay tree on the two small text files (Hamlet and The New Atlantis). While the Java timing indicates Hashtable has better performance than Splay tree for both test, the Unix timing indicated Splay Tree had better performance. This is a relatively small irregularity however, as overall the other tests seemed to match well between Java and Unix timings.

The benchmarking revealed that all methods were overall slower than the BST. Table 11 shows our comparison metric, which indicates all methods besides BST had over 100% values, and thus all were slower than the unweighted BST. Indeed, Tables 9 and 10 show that the only instance of another method, besides the scripts, ever beating BST was the Hashtable on War_and_peace_x9.

The test data also compared the runtime of the provided scripts to the data structure methods. The results showed (Data Table 10), that in every case the perl script beat the shell script. Also, while the scripts beat the BST and data structures for the two small texts, they lost by wide margins on the two large texts.

Additionally, the comparison metrics of the implemented Hashtable and the Java Hashtable were very close (Data Table 11) 115% and 120% compared to BST respectively. Also, the Java Tree implementation and the group's Splay Tree implementation were very close as well, 137% and 137% compared to BST respectively. These close values suggest that the group did in fact implement their data structures very well, because they were as fast as official Java library code.